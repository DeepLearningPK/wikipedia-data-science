{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Book Recommendation System\n",
    "\n",
    "In this notebook, we will build a book recommendation system based on all the books articles on Wikipedia and neural network embeddings of books and links. This is a great look at the potential for neural networks to create meaningful embeddings of high dimensional data and one practical application of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data\n",
    "\n",
    "The data is stored as json with line for every book. This data contains every single book article on Wikipedia which was parsed in the Downloading and Parsing Wikipedia Data Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37020 books.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "books = []\n",
    "\n",
    "with open('found_books_filtered.ndjson', 'r') as fin:\n",
    "    # Append each line to the books\n",
    "    books = [json.loads(l) for l in fin]\n",
    "\n",
    "books_with_wikipedia = [book for book in books if 'Wikipedia:' in book[0]]\n",
    "books = [book for book in books if 'Wikipedia:' not in book[0]]\n",
    "print(f'Found {len(books)} books.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wikipedia:Wikipedia Signpost/2014-06-25/Recent research',\n",
       " 'Wikipedia:New pages patrol/Unpatrolled articles/December 2010',\n",
       " 'Wikipedia:Templates for discussion/Log/2012 September 23',\n",
       " 'Wikipedia:Articles for creation/Redirects/2012-10',\n",
       " 'Wikipedia:Templates for discussion/Log/2012 October 4',\n",
       " 'Wikipedia:Help desk/Archive 63',\n",
       " 'Wikipedia:Teahouse/Questions/Archive 612',\n",
       " 'Wikipedia:List of hoaxes on Wikipedia/La Croix du Sanguine Rouge',\n",
       " \"Wikipedia:Administrators' noticeboard/Archive281\",\n",
       " 'Wikipedia:WikiProject Sailor Moon/Sailor Moon volume 01',\n",
       " 'Wikipedia:Graphics Lab/Illustration workshop/Archive',\n",
       " 'Wikipedia:New pages patrol/Unpatrolled articles/April 2011',\n",
       " 'Wikipedia:Peer review/My Opposition (the Friedrich Kellner diary)/archive1',\n",
       " 'Wikipedia:Manual of Style/Novels',\n",
       " 'Wikipedia:Articles for creation/2007-07-04',\n",
       " 'Wikipedia:WikiProject Warhammer 40,000/Templates',\n",
       " 'Wikipedia:WikiProject Middle-earth/Templates',\n",
       " 'Wikipedia:WikiProject Novels/The Mauritius Command temp',\n",
       " 'Wikipedia:Wikipedia Signpost/2009-11-02/Technology report',\n",
       " 'Wikipedia:New pages patrol/Unpatrolled articles/June 2011']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[book[0] for book in books_with_wikipedia]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each book contains the title, the information from the `Infobox book` template, the internal wikipedia links, the external links, the date of last edit, and the number of characters in the article (a rough estimate of the length of the article)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Derech Mitzvosecha',\n",
       " {'author': 'Rabbi Menachem Mendel Schneersohn ( \" Tzemach Tzedek \" ), the third Rebbe of Chabad',\n",
       "  'caption': 'Derech Mitzvosecha, 1912 edition',\n",
       "  'genre': 'non-fiction',\n",
       "  'image': 'Derech Mitzvosecha 2014-05-08 01-34.jpg',\n",
       "  'image_size': '200px',\n",
       "  'isbn': '0826655904',\n",
       "  'language': 'Hebrew',\n",
       "  'name': 'Derech Mitzvosecha (Sefer Hamitzvos)',\n",
       "  'published': '7x10 Hardcover, 570pp, (Kehot Publication Society, Brooklyn New York)',\n",
       "  'subject': 'Jewish mysticism, Chabad philosophy'},\n",
       " ['Mitzvos',\n",
       "  'Menachem Mendel Schneersohn',\n",
       "  'Rebbe',\n",
       "  'Chabad',\n",
       "  'Hasidic',\n",
       "  'Chabad philosophy',\n",
       "  'Tzitzit',\n",
       "  'Tefillin',\n",
       "  'Hillel the Elder',\n",
       "  'Poltava',\n",
       "  'Kehot Publication Society',\n",
       "  'Eliyahu Touger',\n",
       "  'Sichos in English',\n",
       "  'Tanya',\n",
       "  'Shneur Zalman of Liadi',\n",
       "  'Category:Chabad-Lubavitch (Hasidic dynasty)',\n",
       "  'Category:Books about Judaism',\n",
       "  'Category:Chabad-Lubavitch texts'],\n",
       " ['http://www.books.google.com/books?id=HYLi3l9ylWwC',\n",
       "  'http://www.store.kehotonline.com/index.php?stocknumber=HTZ-DEREM',\n",
       "  'http://www.shturem.org/index.php?section=news',\n",
       "  'http://www.oxfordchabad.org/templates/blog/post_cdo/AID/708481/PostID/36991',\n",
       "  'http://www.chabadlibrary.org/exhibit/ex4/exed5.htm',\n",
       "  'http://www.hebrewbooks.org/15419',\n",
       "  'http://hebrewbooks.org/16082',\n",
       "  'http://www.hebrewbooks.org/15419'],\n",
       " '2018-02-01T01:15:03Z',\n",
       " 4957]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Books to Integers\n",
    "\n",
    "First we want to create a mapping of books to integers. When we feed books into the neural network, we will have to represent them as numbers, so this mapping will let us keep track of the books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22460"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'War and Peace'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_index = {book[0]: idx for idx, book in enumerate(books)}\n",
    "index_book = {idx: book for book, idx in book_index.items()}\n",
    "\n",
    "book_index['War and Peace']\n",
    "index_book[22460]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Unique Wikilinks\n",
    "\n",
    "First we can do a little data exploration. Let's find the number of unique Wikilinks (links to other Wikipedia articles) and the most common ones. To create a single list from a list of lists, we can use the `itertools` chain method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 311276 unique wikilinks.\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "wikilinks = list(chain(*[book[2] for book in books]))\n",
    "print(f\"There are {len(set(wikilinks))} unique wikilinks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of these are links to other books? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17032 unique links to other books.\n"
     ]
    }
   ],
   "source": [
    "wikilinks_other_books = [link for link in wikilinks if link in book_index.keys()]\n",
    "print(f\"There are {len(set(wikilinks_other_books))} unique links to other books.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the counts of each wikilink. We'll make a utility function that takes in a list and returns a sorted ordered dictionary of the counts of the items in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def count_items(l):\n",
    "    \"\"\"Return ordered dictionary of counts of objects in `l`\"\"\"\n",
    "    # Create a counter object\n",
    "    counts = Counter(l)\n",
    "    \n",
    "    # Sort by highest count first\n",
    "    counts = sorted(counts.items(), key = lambda x: x[1], reverse = True)\n",
    "    counts = OrderedDict(counts)\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hardcover', 7641),\n",
       " ('Paperback', 7482),\n",
       " ('Wikipedia:WikiProject Books', 6116),\n",
       " ('Wikipedia:WikiProject Novels', 6087),\n",
       " ('English language', 4307),\n",
       " ('The New York Times', 3960),\n",
       " ('United States', 3388),\n",
       " ('Science fiction', 3121),\n",
       " ('science fiction', 2661),\n",
       " ('Publishers Weekly', 2414)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilink_counts = count_items(wikilinks)\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most linked to pages are in fact not that surprising! One thing we should notice is that there are discrepancies in capitalization. We want to normalize across capitalization, so we'll lowercase all of the links and redo the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 297624 unique wikilinks.\n"
     ]
    }
   ],
   "source": [
    "wikilinks = [link.lower() for link in wikilinks]\n",
    "print(f\"There are {len(set(wikilinks))} unique wikilinks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paperback', 8976),\n",
       " ('hardcover', 8854),\n",
       " ('wikipedia:wikiproject books', 6116),\n",
       " ('wikipedia:wikiproject novels', 6088),\n",
       " ('science fiction', 5917),\n",
       " ('english language', 4371),\n",
       " ('the new york times', 3976),\n",
       " ('united states', 3391),\n",
       " ('novel', 3044),\n",
       " ('publishers weekly', 2414),\n",
       " ('the guardian', 2300),\n",
       " ('fantasy', 2094),\n",
       " ('kirkus reviews', 1741),\n",
       " (\"children's literature\", 1533),\n",
       " ('random house', 1523),\n",
       " ('harpercollins', 1480),\n",
       " ('fantasy novel', 1471),\n",
       " ('doctor who', 1323),\n",
       " ('category:american science fiction novels', 1289),\n",
       " ('non-fiction', 1186)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilink_counts = count_items(wikilinks)\n",
    "list(wikilink_counts.items())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That actually changes the rankings! This illustrates an important point: make sure to take a look at your data before modeling! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the discontinuity guide', 148),\n",
       " ('the encyclopedia of science fiction', 145),\n",
       " ('dracula', 72),\n",
       " ('the dresden files', 70),\n",
       " ('the encyclopedia of fantasy', 66),\n",
       " ('encyclopædia britannica', 61),\n",
       " ('the wonderful wizard of oz', 61),\n",
       " ('nineteen eighty-four', 59),\n",
       " (\"alice's adventures in wonderland\", 54),\n",
       " ('don quixote', 52)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilinks_other_books = [link.lower() for link in wikilinks_other_books]\n",
    "wikilink_books_counts = count_items(wikilinks_other_books)\n",
    "list(wikilink_books_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are so many unique wikilinks, I'm going to limit the list to wikilinks mentioned 5 or more times. Hopefully this reduces the noise that might come from wikilinks that only appear a few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['catherine ryan hyde',\n",
       " 'life with billy (book)',\n",
       " 'shan states',\n",
       " 'burmese chronicles#ramanya',\n",
       " 'northern ndebele people',\n",
       " 'international journal of hindu studies',\n",
       " 'charles henderson (character)',\n",
       " 'anarchy: a journal of desire armed',\n",
       " 'american museum of fly fishing',\n",
       " 'key of perihelion']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = [t[0] for t in wikilink_counts.items() if t[1] >= 5]\n",
    "print(len(links))\n",
    "links[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pound sign in the link name means that the link directs to a specific section on the Wikipedia page. Although we could normalize these to the title of the wikipedia article, we'll leave the pound sign in for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm also going to remove the __most popular__ wikilinks because these are not very informative. If a large number of books have the same wikilink, then the wikilink is not very distinguishing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paperback', 8976),\n",
       " ('hardcover', 8854),\n",
       " ('wikipedia:wikiproject books', 6116),\n",
       " ('wikipedia:wikiproject novels', 6088),\n",
       " ('science fiction', 5917),\n",
       " ('english language', 4371),\n",
       " ('the new york times', 3976),\n",
       " ('united states', 3391),\n",
       " ('novel', 3044),\n",
       " ('publishers weekly', 2414),\n",
       " ('the guardian', 2300),\n",
       " ('fantasy', 2094),\n",
       " ('kirkus reviews', 1741),\n",
       " (\"children's literature\", 1533),\n",
       " ('random house', 1523),\n",
       " ('harpercollins', 1480),\n",
       " ('fantasy novel', 1471),\n",
       " ('doctor who', 1323),\n",
       " ('category:american science fiction novels', 1289),\n",
       " ('non-fiction', 1186)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_remove = ['hardcover', 'paperback', 'wikipedia:wikiproject books', 'wikipedia:wikiproject novels']\n",
    "for t in to_remove:\n",
    "    links.remove(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikilinks to Index\n",
    "\n",
    "As with the books, we need to map the wikilinks to integers. We'll also create the reverse mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'the guardian'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "38550"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_index = {link: idx for idx, link in enumerate(links)}\n",
    "index_link = {idx: link for link, idx in link_index.items()}\n",
    "\n",
    "link_index['the new york times']\n",
    "index_link[6]\n",
    "len(link_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Training Set\n",
    "\n",
    "In order for any machine learning model to learn, it needs a training set. We are going to treat this as a supervised learning problem: given an article, can the neural network determine whether or not a wikilink was present on that page? To do this, we need to create a dictionary of books and the associate links on their page. We already have this in the list of books, but now we need to change our data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book_link_mapping = {book_index[book[0]]: [link_index[link.lower()] for link in book[2] if link.lower() in links] for book in books}\n",
    "\n",
    "# book_link_mapping = {}\n",
    "\n",
    "# for i, book in enumerate(books):\n",
    "#     book_link_mapping[book_index[book[0]]] = []\n",
    "#     for link in book[2]:\n",
    "#         if link.lower() in links:\n",
    "#             book_link_mapping[book_index[book[0]]].append(link_index[link.lower()])\n",
    "    \n",
    "#     if i % 100 == 0:\n",
    "#         print(i, end = '\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each book and each wikilink in the books articles, we'll add it to a list of pairs with the form (book_index, link_index). We also filter out the links that did not have at least 5 occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "\n",
    "for book in books:\n",
    "    pairs.extend((book_index[book[0]], link_index[link.lower()]) for link in book[2] if link.lower() in links)\n",
    "    \n",
    "len(pairs), len(links), len(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 805304 positive examples on which to train! Each pair represents one internal wikipedia link for one book. Let's look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_book[pairs[5000][0]], index_link[pairs[5000][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_book[pairs[10000][0]], index_link[pairs[10000][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Training and Validation\n",
    "\n",
    "For training the neural network, we can split the pairs into a training set and a validation set. We want to make sure that none of the validation set is present in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_set = set(pairs)\n",
    "\n",
    "validation_split = int(0.2 * len(pairs))\n",
    "\n",
    "train_pairs = pairs[validation_split:]\n",
    "train_set = set(train_pairs)\n",
    "\n",
    "valid_pairs = pairs[:validation_split]\n",
    "valid_pairs = [pair for pair in valid_pairs if pair not in train_set]\n",
    "\n",
    "len(train_pairs), len(valid_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Counter(pairs)\n",
    "sorted(x.items(), key = lambda x: x[1], reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"France's Songs of the Bards of the Tyne - 1850\", 'joseph philip robson')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_book[13337], index_link[729]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Early Stories: 1953–1975', 'the new yorker')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_book[31899], index_link[44]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more common wikilinks might be less useful. In a way, this could be similar to problem that creates a need for TF-IDF: the more common a word, the less representative it is of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator For Training Samples\n",
    "\n",
    "We want to generate positive samples and negative samples. The positive samples are simple: pick one random pair from the `pairs` and assign it a 1. The negative samples are also fairly easy: pick one random link and one random book, make sure they are not in the `pairs`, and assign them a -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(100)\n",
    "\n",
    "def generate_batch(pairs, n_positive = 50, negative_ratio = 1.0, accuracy = False):\n",
    "    \"\"\"Generate batches of samples for training\"\"\"\n",
    "    batch_size = n_positive * (1 + negative_ratio)\n",
    "    batch = np.zeros((batch_size, 3))\n",
    "    \n",
    "    if accuracy:\n",
    "        neg_label = 0\n",
    "    else:\n",
    "        neg_label = -1\n",
    "    \n",
    "    while True:\n",
    "        # randomly choose positive examples\n",
    "        for idx, (book_id, link_id) in enumerate(random.sample(pairs, n_positive)):\n",
    "            batch[idx, :] = (book_id, link_id, 1)\n",
    "            \n",
    "        idx = n_positive\n",
    "        \n",
    "        # Add negative examples until reach batch size\n",
    "        while idx < batch_size:\n",
    "            \n",
    "            # random selection\n",
    "            random_book = random.randrange(len(books))\n",
    "            random_link = random.randrange(len(links))\n",
    "            \n",
    "            # Check to make sure this is not a positive example\n",
    "            if (random_book, random_link) not in pairs_set:\n",
    "                batch[idx, :] = (random_book, random_link, neg_label)\n",
    "                idx += 1\n",
    "                \n",
    "        np.random.shuffle(batch)\n",
    "        yield {'book': batch[:, 0], 'link': batch[:, 1]}, batch[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(generate_batch(pairs, n_positive = 2, negative_ratio = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Dot, Reshape, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_embedding_model(embedding_size = 50, accuracy = False):\n",
    "    book = Input(name = 'book', shape = [1])\n",
    "    link = Input(name = 'link', shape = [1])\n",
    "    \n",
    "    book_embedding = Embedding(name = 'book_embedding',\n",
    "                               input_dim = len(book_index),\n",
    "                               output_dim = embedding_size)(book)\n",
    "    \n",
    "    link_embedding = Embedding(name = 'link_embedding',\n",
    "                               input_dim = len(link_index),\n",
    "                               output_dim = embedding_size)(link)\n",
    "    \n",
    "    book_embedding = Reshape(target_shape = [embedding_size])(book_embedding)\n",
    "    link_embedding = Reshape(target_shape = [embedding_size])(link_embedding)\n",
    "    \n",
    "    merged = Dot(name = 'dot_product', normalize = True, axes = 1)([book_embedding, link_embedding])\n",
    "    \n",
    "    if accuracy:\n",
    "        out = Dense(1, activation = 'sigmoid')(merged)\n",
    "        model = Model(inputs = [book, link], outputs = out)\n",
    "        model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    else:\n",
    "#         merged = Dense(1, activation = None)(merged)\n",
    "        model = Model(inputs = [book, link], outputs = merged)\n",
    "        model.compile(optimizer = 'Adam', loss = 'mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = book_embedding_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "\n",
    "The next step is just to train the model. We can fit the model on the generator and also make a generator for validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pairs) / 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_positive = 512\n",
    "\n",
    "train_gen = generate_batch(train_pairs, n_positive, negative_ratio=5)\n",
    "valid_gen = generate_batch(valid_pairs, n_positive, negative_ratio=5)\n",
    "\n",
    "model.fit_generator(train_gen, epochs = 20, \n",
    "                    steps_per_epoch = len(pairs) // n_positive,\n",
    "                    validation_data = valid_gen, \n",
    "                    validation_steps = 10,\n",
    "                    verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('first_attempt.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_layer = model.get_layer('book_embedding')\n",
    "book_weights = book_layer.get_weights()[0]\n",
    "book_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_weights = book_weights / np.linalg.norm(book_weights, axis = 1).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.square(book_weights[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_books(book, book_weights, n = 10, least = False):\n",
    "    dists = np.dot(book_weights, book_weights[book_index[book]])\n",
    "    \n",
    "    if least:\n",
    "        closest = np.argsort(dists)[:n]\n",
    "        \n",
    "    else:\n",
    "        closest = np.argsort(dists)[-n:]\n",
    "    \n",
    "    for c in reversed(closest):\n",
    "        print('Book:', index_book[c], 'Distance:', dists[c])\n",
    "    \n",
    "    return closest, dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = find_similar_books('War and Peace', book_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = find_similar_books('War and Peace', book_weights, least = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = find_similar_books('The Autobiography of Benjamin Franklin', book_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = find_similar_books('A More Perfect Constitution', book_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[-10:], y[x[-10:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model For Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_acc = book_embedding_model(50, True)\n",
    "\n",
    "train_gen_acc = generate_batch(train_pairs, n_positive=512, negative_ratio=1, accuracy = True)\n",
    "valid_gen_acc = generate_batch(valid_pairs, n_positive=512, negative_ratio=1, accuracy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_acc.fit_generator(train_gen_acc, epochs = 10, steps_per_epoch=1000,\n",
    "                        validation_data = valid_gen_acc, \n",
    "                        validation_steps = 10, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_layer_acc = model_acc.get_layer('book_embedding')\n",
    "book_weights_acc = book_layer_acc.get_weights()[0]\n",
    "\n",
    "book_weights_acc = book_weights_acc / np.linalg.norm(book_weights_acc, axis = 1).reshape((-1, 1))\n",
    "\n",
    "book_weights_acc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = find_similar_books('War and Peace', book_weights_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = books[book_index['War and Peace']][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = books[book_index['Son of Spellsinger']][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in l1:\n",
    "    if l in l2:\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = find_similar_books('A Storm of Swords', book_weights_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('first_attempt_laptop.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "Of course the fun part is making predictions with the model. We can feed it any book and get the closest and furthest away books in the embedding space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
