{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Book Recommendation System\n",
    "\n",
    "In this notebook, we will build a book recommendation system based on a simple principle: books that link to the same Wikipedia pages are alike. In order to create this representation of similar books, we'll use the concept of neural network entity embeddings, mapping each book and each Wikipedia link (Wikilink) to a 50 number vector. The idea of entity embeddings is to map high-dimensional categorical variables to a low-dimensional _learned_ vector that _places similar entities closer together in the embedding space_. If we were to one-hot-encode the books (another representation of categorical data) we would have a 37,000 dimension vector for each book, with a single 1 indicating the book and similar books would not be \"closer\" to one another. By  training a neural network to learn entity embeddings, we not only get a reduced dimension representation of the books, we also get a representation that _keeps similar books closer to each other_. Therefore, the basic approach is to create entity embeddings of all the books, and then for any book, find the closest book in the embedding space as a recommendation. [Fortunately, we have access to every single book on Wikipedia](https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/Downloading%20and%20Parsing%20Wikipedia%20Articles.ipynb), which will let us create a comprehensive system.\n",
    "\n",
    "## Approach\n",
    "\n",
    "To create an entity embedding, we train a neural network for a supervised machine learning task and tell it to represent the books and the links as vectors that can be adjusted in order to improve the predictions. In other words, the parameters of the model are the embeddings meaning that the network tries to learn the task by changing the representations of the books and the links. Once we have the embeddings for the books and the links, we can find the most similar book to a given book by computing the distance between the vector for that book and all the other books. We'll use the cosine distance which measures the angle between two vectors as a measure of similarity (another valid option is the Euclidean distance). We can also do the same with the links, finding the most similar page to a given page that occurs in the links.\n",
    "\n",
    "### Supervised Machine Learning Task: Map Books to Links\n",
    "\n",
    "For our machine learning task, we'll set up the problem as identifying whether or not a particular link was present in a book article. The training examples will consist of (book, link) pairs, with some pairs true examples - actually  in the data - and others negative examples that do not occur in the data. It will be the network's job to adjust the entity embeddings of the books and the links in order to accurately make this classification.\n",
    "\n",
    "## Entity Embeddings\n",
    "\n",
    "Entity embeddings have proven to be very powerful concepts for modeling language and categorical variables. For example, the Word2Vec embeddings map a word to a 50 dimensional vector based on training a neural network on millions of words. Embeddings can then be used in any supervised model, because they are just representations of categorical variables. Much as we might one-hot-encode categorical variables to use them in a random forest for a supervised task, we can also use the entity embeddings to include categorical variables in any model. \n",
    "\n",
    "We can also use the Entity Embeddings to visualize concepts, such as creating a map of all books on Wikipedia. The entity embeddings typically are still high-dimensional - we'll use 50 numbers for each entity - so we need to use a dimension reduction technique such as TSNE or UMAP to visualize the embeddings in lower dimensions. We'll take a look at doing this at the end of the notebook and later will upload the embeddings into a application custom-built for this purpose ([projector.tensorflow.org](https://projector.tensorflow.org)). Entity embeddings are becoming more widespread thanks to the ease of development of neural networks in Keras and are a worthwhile approach when we want to represent categorical variables in relevant numbers. Other approaches for encoding categorical variables do not represent similar entities as being closer to one another, and entity embedding is a learning-based method for this crucial task.\n",
    "\n",
    "Overall, this project is a great look at the potential for neural networks to create meaningful embeddings of high dimensional data and a practical application of deep learning. The code itself is relatively simple, and the Keras library makes developing deep learning models enjoyable!\n",
    "\n",
    "The code here is adapted from the excellent [Deep Learning Cookbook](http://shop.oreilly.com/product/0636920097471.do), the [notebooks for which can be found on GitHub](https://github.com/DOsinga/deep_learning_cookbook). Check out this book for practical applications of deep learning and great projects! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data\n",
    "\n",
    "The data is stored as json with line for every book. This data contains every single book article on Wikipedia which was parsed in the [Downloading and Parsing Wikipedia Data Notebook](https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/Downloading%20and%20Parsing%20Wikipedia%20Articles.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# Set shell to show all lines of output\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37020 books.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "books = []\n",
    "\n",
    "with open('found_books_filtered.ndjson', 'r') as fin:\n",
    "    # Append each line to the books\n",
    "    books = [json.loads(l) for l in fin]\n",
    "\n",
    "# Remove non-book articles\n",
    "books_with_wikipedia = [book for book in books if 'Wikipedia:' in book[0]]\n",
    "books = [book for book in books if 'Wikipedia:' not in book[0]]\n",
    "print(f'Found {len(books)} books.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few articles that were caught which are clearly not books (feel free to check out these articles yourself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wikipedia:Wikipedia Signpost/2014-06-25/Recent research',\n",
       " 'Wikipedia:New pages patrol/Unpatrolled articles/December 2010',\n",
       " 'Wikipedia:Templates for discussion/Log/2012 September 23',\n",
       " 'Wikipedia:Articles for creation/Redirects/2012-10',\n",
       " 'Wikipedia:Templates for discussion/Log/2012 October 4']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[book[0] for book in books_with_wikipedia][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each legitimate book contains the __title, the information from the `Infobox book` template, the internal wikipedia links, the external links, the date of last edit, and the number of characters in the article__ (a rough estimate of the length of the article)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Limonov (novel)',\n",
       " {'author': 'Emmanuel Carrère',\n",
       "  'country': 'France',\n",
       "  'english_pub_date': '2014',\n",
       "  'isbn': '978-2-8180-1405-9',\n",
       "  'language': 'French',\n",
       "  'name': 'Limonov',\n",
       "  'pages': '488',\n",
       "  'pub_date': '2011',\n",
       "  'publisher': 'P.O.L.',\n",
       "  'translator': 'John Lambert'},\n",
       " ['Emmanuel Carrère',\n",
       "  'biographical novel',\n",
       "  'Emmanuel Carrère',\n",
       "  'Eduard Limonov',\n",
       "  'Prix de la langue française'],\n",
       " ['http://www.lefigaro.fr/flash-actu/2011/10/05/97001-20111005FILWWW00615-le-prix-de-la-langue-francaise-a-e-carrere.php',\n",
       "  'http://www.lexpress.fr/culture/livre/emmanuel-carrere-prix-renaudot-2011_1046819.html',\n",
       "  'http://limonow.de/carrere/index.html',\n",
       "  'http://www.tout-sur-limonov.fr/222318809'],\n",
       " ['http://www.lefigaro.fr/flash-actu/2011/10/05/97001-20111005FILWWW00615-le-prix-de-la-langue-francaise-a-e-carrere.php',\n",
       "  'http://www.lexpress.fr/culture/livre/emmanuel-carrere-prix-renaudot-2011_1046819.html',\n",
       "  'http://limonow.de/carrere/index.html',\n",
       "  'http://www.tout-sur-limonov.fr/222318809'],\n",
       " '2018-08-18T02:03:21Z',\n",
       " 1437)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 21\n",
    "books[n][0], books[n][1], books[n][2][:5], books[n][3][:5], books[n][3][:5], books[n][4], books[n][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Books to Integers\n",
    "\n",
    "First we want to create a mapping of books to integers. When we feed books into the neural network for the embedding, we will have to represent them as numbers, so this mapping will let us keep track of the books. We'll also create the reverse mapping, from integers back to the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22494"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Anna Karenina'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_index = {book[0]: idx for idx, book in enumerate(books)}\n",
    "index_book = {idx: book for book, idx in book_index.items()}\n",
    "\n",
    "book_index['Anna Karenina']\n",
    "index_book[22494]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Wikilinks\n",
    "\n",
    "Although it's not our main focus, we can do a little exploration. Let's find the number of unique Wikilinks  and the most common ones. To create a single list from a list of lists, we can use the `itertools` chain method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 311276 unique wikilinks.\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "wikilinks = list(chain(*[book[2] for book in books]))\n",
    "print(f\"There are {len(set(wikilinks))} unique wikilinks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of these are links to other books? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17032 unique links to other books.\n"
     ]
    }
   ],
   "source": [
    "wikilinks_other_books = [link for link in wikilinks if link in book_index.keys()]\n",
    "print(f\"There are {len(set(wikilinks_other_books))} unique links to other books.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the counts of each wikilink. We'll make a utility function that takes in a list and returns a sorted ordered dictionary of the counts of the items in the list. The `collections` module has a number of useful functions for dealing with groups of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def count_items(l):\n",
    "    \"\"\"Return ordered dictionary of counts of objects in `l`\"\"\"\n",
    "    \n",
    "    # Create a counter object\n",
    "    counts = Counter(l)\n",
    "    \n",
    "    # Sort by highest count first and place in ordered dictionary\n",
    "    counts = sorted(counts.items(), key = lambda x: x[1], reverse = True)\n",
    "    counts = OrderedDict(counts)\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hardcover', 7641),\n",
       " ('Paperback', 7482),\n",
       " ('Wikipedia:WikiProject Books', 6116),\n",
       " ('Wikipedia:WikiProject Novels', 6087),\n",
       " ('English language', 4307),\n",
       " ('The New York Times', 3960),\n",
       " ('United States', 3388),\n",
       " ('Science fiction', 3121),\n",
       " ('science fiction', 2661),\n",
       " ('Publishers Weekly', 2414)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilink_counts = count_items(wikilinks)\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most linked to pages are in fact not that surprising! One thing we should notice is that there are discrepancies in capitalization. We want to normalize across capitalization, so we'll lowercase all of the links and redo the counts.\n",
    "\n",
    "\n",
    "### Data Cleaning of Wikilinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 297624 unique wikilinks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('paperback', 8976),\n",
       " ('hardcover', 8854),\n",
       " ('wikipedia:wikiproject books', 6116),\n",
       " ('wikipedia:wikiproject novels', 6088),\n",
       " ('science fiction', 5917),\n",
       " ('english language', 4371),\n",
       " ('the new york times', 3976),\n",
       " ('united states', 3391),\n",
       " ('novel', 3044),\n",
       " ('publishers weekly', 2414)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilinks = [link.lower() for link in wikilinks]\n",
    "print(f\"There are {len(set(wikilinks))} unique wikilinks.\")\n",
    "\n",
    "wikilink_counts = count_items(wikilinks)\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That actually changes the rankings! This illustrates an important point: __make sure to take a look at your data before modeling!__ \n",
    "\n",
    "I'm going to remove the __most popular__ wikilinks because these are not very informative. Knowing whether a book is hardcover or paperback is not that important to the content. We also don't need the two `Wikipedia:` links since these do not distinguish the books based on content. __I'd recommend playing around with this list because it might have a large effect on the recommendations.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['hardcover', 'paperback', 'wikipedia:wikiproject books', 'wikipedia:wikiproject novels']\n",
    "for t in to_remove:\n",
    "    links.remove(t)\n",
    "    _ = wikilink_counts.pop(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are so many unique wikilinks, I'm going to limit the list to wikilinks mentioned 5 or more times. Hopefully this reduces the noise that might come from wikilinks that only appear a few times. Keeping every single link leads to unreasonable neural network training time, but you are welcome to try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38550\n"
     ]
    }
   ],
   "source": [
    "# Limit to greater than 4 links\n",
    "links = [t[0] for t in wikilink_counts.items() if t[1] >= 5]\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final bit of exploration, let's look at the books that are mentioned the most by other books on Wikipedia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Discontinuity Guide', 148),\n",
       " ('The Encyclopedia of Science Fiction', 145),\n",
       " ('Dracula', 72),\n",
       " ('The Dresden Files', 70),\n",
       " ('The Encyclopedia of Fantasy', 66),\n",
       " ('Encyclopædia Britannica', 61),\n",
       " ('The Wonderful Wizard of Oz', 61),\n",
       " ('Nineteen Eighty-Four', 59),\n",
       " (\"Alice's Adventures in Wonderland\", 54),\n",
       " ('Don Quixote', 52)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilink_books_counts = count_items(wikilinks_other_books)\n",
    "list(wikilink_books_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not surprising that several of these are references. We also see that a few classics make it into the list! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikilinks to Index\n",
    "\n",
    "As with the books, we need to map the Wikilinks to integers. We'll also create the reverse mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'the economist'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "38550"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_index = {link: idx for idx, link in enumerate(links)}\n",
    "index_link = {idx: link for link, idx in link_index.items()}\n",
    "\n",
    "link_index['the economist']\n",
    "index_link[240]\n",
    "len(link_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Training Set\n",
    "\n",
    "In order for any machine learning model to learn, it needs a training set. We are going to treat this as a supervised learning problem: given a pair (book, link), we want the neural network to learn to predict whether this is a legitimate pair - present in the data - or not.\n",
    "\n",
    "For each book, we'll iterate through the wikilinks on the book page and save them to a list. The final `pairs` list will consist of tuples of every (book, link) pairing on all of Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768821, 38550, 37020)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = []\n",
    "\n",
    "# Iterate through each book\n",
    "for book in books:\n",
    "    # Iterate through the links in the book\n",
    "    pairs.extend((book_index[book[0]], link_index[link.lower()]) for link in book[2] if link.lower() in links)\n",
    "    \n",
    "len(pairs), len(links), len(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have over 750,000 positive examples on which to train! Each pair represents one Wikilink for one book. Let's look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Dr. Seuss's ABC\", 'dr. seuss')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_book[pairs[5000][0]], index_link[pairs[5000][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Man Who Watched the Trains Go By (novel)',\n",
       " 'category:belgian novels adapted into films')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_book[pairs[900][0]], index_link[pairs[900][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on we'll create the negative examples by randomly sampling from the links and the books and making sure the resulting pair is not in `pairs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_set = set(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note about Training / Testing Set\n",
    "\n",
    "To compute the embeddings, we are not going to create a separate validation or testing set. While this is a __must__ for a normal supervised machine learning task, in this case, our primary objective is not to make the most accurate model, but to generate the best embeddings. The prediction task is just the method through which we train our network to make the embeddings. At the end of training, we are not going to be testing our model on new data, so we don't need to evaluate the performance. Instead of testing on new data, we'll look at the embeddings themselves to see if books that we think are similar have embeddings that are close to each other. \n",
    "\n",
    "If we kept a separate validation / testing set, then we would be limiting the amount of data that our network can use to train. This would result in less accurate embeddings. Normally with any supervised model, we need to be concerned about overfitting, but again, because we do not need our model to generalize to new data and our goal is the embeddings, we will make our model as effective as possible by using all the data for training. In general, always have a separate validation and testing set (or use cross validation) and make sure to regularize your model to prevent overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's look at the (book, link) pairs that are represented most often in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((13337, 725), 85),\n",
       " ((31899, 40), 77),\n",
       " ((5919, 1884), 62),\n",
       " ((25899, 1395), 61),\n",
       " ((10394, 1955), 60)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Counter(pairs)\n",
    "sorted(x.items(), key = lambda x: x[1], reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"France's Songs of the Bards of the Tyne - 1850\", 'joseph philip robson')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('The Early Stories: 1953–1975', 'the new yorker')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('Registry of World Record Size Shells', 'gastropoda')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_book[13337], index_link[725]\n",
    "index_book[31899], index_link[40]\n",
    "index_book[10394], index_link[1955]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's nothing wrong with books that link to the same page many times. They are just more likely to be trained on since there are more of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator For Training Samples\n",
    "\n",
    "We want to generate positive samples and negative samples. The positive samples are simple: pick one random pair from the `pairs` and assign it a 1. The negative samples are also fairly easy: pick one random link and one random book, make sure they are not in the `pairs`, and assign them a -1 or a 0. We'll use either a 0 or -1 for the negative labels depending on whether we want to make this a regression or a classification problem. Either approach is valid, and we'll try out both methods.\n",
    "\n",
    "The code below creates a generator that yields batches of samples. Each time we call `next` on the generator, it `yield`s another batch. Neural networks are trained incrementally - a batch at a time - which means that a generator is a useful function for returning examples on which to train. This alleviates the need to store all of the training data in memory which might be an issue if we were working with a larger dataset such as images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(100)\n",
    "\n",
    "def generate_batch(pairs, n_positive = 50, negative_ratio = 1.0, classification = False):\n",
    "    \"\"\"Generate batches of samples for training\"\"\"\n",
    "    batch_size = n_positive * (1 + negative_ratio)\n",
    "    batch = np.zeros((batch_size, 3))\n",
    "    \n",
    "    # Adjust label based on task\n",
    "    if classification:\n",
    "        neg_label = 0\n",
    "    else:\n",
    "        neg_label = -1\n",
    "    \n",
    "    # This creates a generator\n",
    "    while True:\n",
    "        # randomly choose positive examples\n",
    "        for idx, (book_id, link_id) in enumerate(random.sample(pairs, n_positive)):\n",
    "            batch[idx, :] = (book_id, link_id, 1)\n",
    "\n",
    "        # Increment idx by 1\n",
    "        idx += 1\n",
    "        \n",
    "        # Add negative examples until reach batch size\n",
    "        while idx < batch_size:\n",
    "            \n",
    "            # random selection\n",
    "            random_book = random.randrange(len(books))\n",
    "            random_link = random.randrange(len(links))\n",
    "            \n",
    "            # Check to make sure this is not a positive example\n",
    "            if (random_book, random_link) not in pairs_set:\n",
    "                \n",
    "                # Add to batch and increment index\n",
    "                batch[idx, :] = (random_book, random_link, neg_label)\n",
    "                idx += 1\n",
    "                \n",
    "        # Make sure to shuffle order\n",
    "        np.random.shuffle(batch)\n",
    "        yield {'book': batch[:, 0], 'link': batch[:, 1]}, batch[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a new batch, call `yield` on the generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'book': array([ 6917., 25757., 28410., 22239., 29814.,  7206.]),\n",
       "  'link': array([21344., 22920., 33217.,  6046., 11452., 34924.])},\n",
       " array([ 1., -1., -1.,  1., -1., -1.]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(generate_batch(pairs, n_positive = 2, negative_ratio = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book: Des Imagistes             Link: journal of asian studies                 Label: -1.0\n",
      "Book: Soul Music (novel)        Link: bermuda triangle                         Label: -1.0\n",
      "Book: Black Skin, White Masks   Link: nationalism                              Label: 1.0\n",
      "Book: The Soul of the Robot     Link: allanon                                  Label: -1.0\n",
      "Book: The Counterfeit Man       Link: julie andrews                            Label: -1.0\n",
      "Book: Lord of the World         Link: prime minister of the united kingdom     Label: 1.0\n"
     ]
    }
   ],
   "source": [
    "x, y = next(generate_batch(pairs, n_positive = 2, negative_ratio = 2))\n",
    "\n",
    "for lab, b_idx, l_idx in zip(y, x['book'], x['link']):\n",
    "    print(f'Book: {index_book[b_idx]:25} Link: {index_link[l_idx]:40} Label: {lab}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Embedding Model\n",
    "\n",
    "With our training samples generator, we're almost there. The next step is the most technically complicated but thankfully made much easier with Keras. We are going to construct the neural network that learns the entity embeddings. The input to this network is the [book, link] as integers, and the output will be a prediction of whether or not the link was present in the book article. The heart of the network is the embedding layers, one for the book and one for the link each of which maps the input entity to a 50 dimensional vector. \n",
    "\n",
    "After converting to the embedding, we need a way to combine the embeddings into a single number. For this we can use the dot product which does element-wise multiplication of numbers in the vectors and then sums the result to a single number. This number is then the ouput of the model for the case of regression. In regression, our labels are either -1 or 1, and so the model loss function will be mean squared error in order to minimize the distance between the prediction and the output. \n",
    "\n",
    "For classification, we add an extra fully connected `Dense` layer with a `sigmoid` activation to squash the outputs between 0 and 1 because the labels are either 0 or 1. The loss function for classification is `binary_crossentropy` which measures the [error of the neural network predictions in a binary classification problem](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html), and is a measure of the similarity between two distributions. \n",
    "\n",
    "The optimizer - the algorithm used to update the parameters of the neural network through backpropagation - is Adam in both cases. We use the default parameters for this optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willk\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, Dot, Reshape, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "book (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "link (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "book_embedding (Embedding)      (None, 1, 50)        1851000     book[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "link_embedding (Embedding)      (None, 1, 50)        1927500     link[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 50)           0           book_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 50)           0           link_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dot_product (Dot)               (None, 1)            0           reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,778,500\n",
      "Trainable params: 3,778,500\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def book_embedding_model(embedding_size = 50, classification = False):\n",
    "    \"\"\"Model to embed books and wikilinks using the functional API.\n",
    "       Trained to discern if a link is present in a article\"\"\"\n",
    "    \n",
    "    # Both inputs are 1-dimensional\n",
    "    book = Input(name = 'book', shape = [1])\n",
    "    link = Input(name = 'link', shape = [1])\n",
    "    \n",
    "    # Embedding the book\n",
    "    book_embedding = Embedding(name = 'book_embedding',\n",
    "                               input_dim = len(book_index),\n",
    "                               output_dim = embedding_size)(book)\n",
    "    \n",
    "    # Embedding the link\n",
    "    link_embedding = Embedding(name = 'link_embedding',\n",
    "                               input_dim = len(link_index),\n",
    "                               output_dim = embedding_size)(link)\n",
    "    \n",
    "    # Reshape to be a single embedding size vector\n",
    "    book_embedding = Reshape(target_shape = [embedding_size])(book_embedding)\n",
    "    link_embedding = Reshape(target_shape = [embedding_size])(link_embedding)\n",
    "    \n",
    "    # Merge the layers with a dot product\n",
    "    merged = Dot(name = 'dot_product', normalize = True, axes = 1)([book_embedding, link_embedding])\n",
    "    \n",
    "    # If classifcation, add extra layer and loss function is binary cross entropy\n",
    "    if classification:\n",
    "        merged = Dense(1, activation = 'sigmoid')(merged)\n",
    "        model = Model(inputs = [book, link], outputs = merged)\n",
    "        model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    # Otherwise loss function is mean squared error\n",
    "    else:\n",
    "        model = Model(inputs = [book, link], outputs = merged)\n",
    "        model.compile(optimizer = 'Adam', loss = 'mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate model and show parameters\n",
    "model = book_embedding_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that during training, there are nearly 3.8 million weights that need to be learned by the neural network. Each of these represents one number in an embedding for one entity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "\n",
    "We have the training data - in a generator - and a model. The next step is to train the model to learn the entity embeddings. During this process, the model will update the embeddings in order to accomplish the task or predicting whether a certain link is actually on a book page or not. The resulting embeddings can then be used as a representation of books and links. \n",
    "\n",
    "There are a few parameters to adjust for training. The batch size should generally be as large as possible given the memory constraints of your machine. The negative ratio can be adjusted based on results. I tried 2 and it seemed to work well. The number of steps per epoch is chosen such that the model sees a number of examples equal to the number of pairs on each epoch. This is repeated for 20 epochs (which might be more than necessary).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 31s - loss: 0.3162\n",
      "Epoch 2/20\n",
      " - 31s - loss: 0.3149\n",
      "Epoch 3/20\n",
      " - 32s - loss: 0.3179\n",
      "Epoch 4/20\n",
      " - 32s - loss: 0.3193\n",
      "Epoch 5/20\n",
      " - 32s - loss: 0.3171\n",
      "Epoch 6/20\n",
      " - 31s - loss: 0.3161\n",
      "Epoch 7/20\n",
      " - 32s - loss: 0.3158\n",
      "Epoch 8/20\n",
      " - 31s - loss: 0.3141\n",
      "Epoch 9/20\n",
      " - 31s - loss: 0.3132\n",
      "Epoch 10/20\n",
      " - 32s - loss: 0.3128\n",
      "Epoch 11/20\n",
      " - 31s - loss: 0.3129\n",
      "Epoch 12/20\n",
      " - 32s - loss: 0.3144\n",
      "Epoch 13/20\n",
      " - 32s - loss: 0.3124\n",
      "Epoch 14/20\n",
      " - 32s - loss: 0.3136\n",
      "Epoch 15/20\n",
      " - 34s - loss: 0.3149\n",
      "Epoch 16/20\n",
      " - 36s - loss: 0.3135\n",
      "Epoch 17/20\n",
      " - 35s - loss: 0.3128\n",
      "Epoch 18/20\n",
      " - 37s - loss: 0.3140\n",
      "Epoch 19/20\n",
      " - 35s - loss: 0.3120\n",
      "Epoch 20/20\n",
      " - 38s - loss: 0.3122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27d07f1d2e8>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_positive = 512\n",
    "\n",
    "gen = generate_batch(pairs, n_positive, negative_ratio = 5)\n",
    "\n",
    "model.fit_generator(gen, epochs = 20, \n",
    "                    steps_per_epoch = len(pairs) // n_positive,\n",
    "                    verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire trained model can be saved and later loaded in. It's also possible to save certain layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('first_attempt.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Embeddings\n",
    "\n",
    "The trained model has learned - hopefully - representations of books and wikilinks that place similar entities next to one another. To find out if this is the case, we extract the embeddings and use them to find similar books and links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37020, 50)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract embeddings\n",
    "book_layer = model.get_layer('book_embedding')\n",
    "book_weights = book_layer.get_weights()[0]\n",
    "book_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to normalize the embeddings so that the dot product between two embeddings becomes the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_weights = book_weights / np.linalg.norm(book_weights, axis = 1).reshape((-1, 1))\n",
    "np.sum(np.square(book_weights[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize just means divide each vector by the square root of the sum of squared components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Find Most Similar Entities\n",
    "\n",
    "The function below takes in either a book or a link, a set of embeddings, and returns the `n` most similar items to the query. It does this by computing the dot product between the query and embeddings. Because we normalized the embeddings, the dot product represents the cosine similarity between two vectors. This is a measure of similarity that does not depend on the magnitude of the vector as does the Euclidean distance. The Euclidean distance would be another valid metric of similary to use to compare the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(name, weights, index_name = 'book', n = 10, least = False, return_dist = False):\n",
    "    \"\"\"Find n most similar items (or least) to name based on embeddings\"\"\"\n",
    "    \n",
    "    # Select index and reverse index\n",
    "    if index_name == 'book':\n",
    "        index = book_index\n",
    "        rindex = index_book\n",
    "    elif index_name == 'link':\n",
    "        index = link_index\n",
    "        rindex = index_link\n",
    "    \n",
    "    # Check to make sure `name` is in index\n",
    "    try:\n",
    "        # Calculate dot product between book and all others\n",
    "        dists = np.dot(weights, weights[index[name]])\n",
    "    except KeyError:\n",
    "        print(f'{name} Not Found.')\n",
    "        return\n",
    "    \n",
    "    # If specified, find the least similar\n",
    "    if least:\n",
    "        # Sort the indices from smallest to largest and take the first n\n",
    "        closest = np.argsort(dists)[:n]\n",
    "        \n",
    "    # Otherwise find the most similar\n",
    "    else:\n",
    "        # Sort the indices from smallest to largest and take the last n\n",
    "        closest = np.argsort(dists)[-n:]\n",
    "    \n",
    "    # Need distances later on\n",
    "    if return_dist:\n",
    "        return dists, closest\n",
    "    \n",
    "    # Print formatting\n",
    "    max_width = max([len(rindex[c]) for c in closest])\n",
    "    \n",
    "    # Print the most similar and distances\n",
    "    for c in reversed(closest):\n",
    "        print(f'{index_name.capitalize()}: {rindex[c]:{max_width + 2}} Similarity: {dists[c]:.{2}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can know that this function works if the most similar book is the book itself. Because we multiply the item vector times all the other embeddings, the most similar should be the item itself with a similarity of 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book: War and Peace               Similarity: 1.0\n",
      "Book: Anna Karenina               Similarity: 0.95\n",
      "Book: The Master and Margarita    Similarity: 0.91\n",
      "Book: Candide                     Similarity: 0.89\n",
      "Book: Poor Folk                   Similarity: 0.88\n",
      "Book: Demons (Dostoevsky novel)   Similarity: 0.88\n",
      "Book: Buddenbrooks                Similarity: 0.87\n",
      "Book: Crime and Punishment        Similarity: 0.86\n",
      "Book: Dead Souls                  Similarity: 0.85\n",
      "Book: The Good Soldier Švejk      Similarity: 0.85\n"
     ]
    }
   ],
   "source": [
    "find_similar('War and Peace', book_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! The most similar books make sense at least for War and Peace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book: Crystal Mask                 Similarity: -0.35\n",
      "Book: List of Transformers books   Similarity: -0.36\n",
      "Book: Dungeon Master's Guide       Similarity: -0.38\n",
      "Book: Kushiel's Justice            Similarity: -0.39\n",
      "Book: Player's Handbook            Similarity: -0.4\n"
     ]
    }
   ],
   "source": [
    "find_similar('War and Peace', book_weights, least = True, n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least similar books are quite a grab bag!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book: The Autobiography of Benjamin Franklin           Similarity: 1.0\n",
      "Book: Pudd'nhead Wilson                                Similarity: 0.94\n",
      "Book: Cloudsplitter                                    Similarity: 0.93\n",
      "Book: Poems on Various Subjects, Religious and Moral   Similarity: 0.93\n",
      "Book: Letters from an American Farmer                  Similarity: 0.93\n"
     ]
    }
   ],
   "source": [
    "find_similar('The Autobiography of Benjamin Franklin', book_weights, n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't work perfectly for every book as we can see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book: A More Perfect Constitution                            Similarity: 1.0\n",
      "Book: Never Trust a Liberal Over 3—Especially a Republican   Similarity: 0.99\n",
      "Book: Government Bullies                                     Similarity: 0.99\n",
      "Book: National Security and Double Government                Similarity: 0.99\n",
      "Book: Culture of Corruption                                  Similarity: 0.99\n"
     ]
    }
   ],
   "source": [
    "find_similar('A More Perfect Constitution', book_weights, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book: The Fellowship of the Ring   Similarity: 1.0\n",
      "Book: The Return of the King       Similarity: 0.98\n",
      "Book: Beren and Lúthien            Similarity: 0.96\n",
      "Book: The Children of Húrin        Similarity: 0.95\n",
      "Book: The Silmarillion             Similarity: 0.95\n"
     ]
    }
   ],
   "source": [
    "find_similar('The Fellowship of the Ring', book_weights, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book: The Two Towers                Similarity: 1.0\n",
      "Book: Beren and Lúthien             Similarity: 0.96\n",
      "Book: Morgoth's Ring                Similarity: 0.96\n",
      "Book: The History of Middle-earth   Similarity: 0.96\n",
      "Book: The Book of Lost Tales        Similarity: 0.96\n"
     ]
    }
   ],
   "source": [
    "find_similar('The Two Towers', book_weights, n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad by the looks of the results! I encourage you to play around with the model and explore the resulting embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book: Bully for Brontosaurus          Similarity: 1.0\n",
      "Book: I Have Landed                   Similarity: 0.98\n",
      "Book: An Urchin in the Storm          Similarity: 0.97\n",
      "Book: Eight Little Piggies            Similarity: 0.97\n",
      "Book: Biology for Christian Schools   Similarity: 0.97\n"
     ]
    }
   ],
   "source": [
    "find_similar('Bully for Brontosaurus', book_weights, n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Embeddings\n",
    "\n",
    "We also have the embeddings of wikipedia links. We can take a similar approach to extract these and find the most similar to a query link. \n",
    "\n",
    "Let's write a quick function to extract weights from a model given the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weights(name, model):\n",
    "    \"\"\"Extract weights from a neural network model\"\"\"\n",
    "    \n",
    "    # Extract weights\n",
    "    weight_layer = model.get_layer(name)\n",
    "    weights = weight_layer.get_weights()[0]\n",
    "    \n",
    "    # Normalize\n",
    "    weights = weights / np.linalg.norm(weights, axis = 1).reshape((-1, 1))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_weights = extract_weights('link_embedding', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same find similar function to find the most similar links to a given link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link: science fiction                            Similarity: 1.0\n",
      "Link: category:american science fiction novels   Similarity: 0.94\n",
      "Link: fantasy                                    Similarity: 0.88\n",
      "Link: tor books                                  Similarity: 0.85\n",
      "Link: united states                              Similarity: 0.85\n",
      "Link: english language                           Similarity: 0.83\n",
      "Link: category:doubleday (publisher) books       Similarity: 0.83\n",
      "Link: doubleday (publisher)                      Similarity: 0.83\n",
      "Link: novel                                      Similarity: 0.83\n",
      "Link: bantam books                               Similarity: 0.8\n"
     ]
    }
   ],
   "source": [
    "find_similar('science fiction', link_weights, index_name = 'link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link: biography                             Similarity: 1.0\n",
      "Link: autobiography                         Similarity: 0.9\n",
      "Link: category:american non-fiction books   Similarity: 0.86\n",
      "Link: non-fiction                           Similarity: 0.85\n",
      "Link: category:english-language books       Similarity: 0.83\n",
      "Link: category:2005 books                   Similarity: 0.83\n",
      "Link: memoir                                Similarity: 0.83\n",
      "Link: category:2010 non-fiction books       Similarity: 0.81\n",
      "Link: category:2008 books                   Similarity: 0.8\n",
      "Link: category:2006 books                   Similarity: 0.8\n"
     ]
    }
   ],
   "source": [
    "find_similar('biography', link_weights, index_name = 'link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link: eos books                             Similarity: -0.76\n",
      "Link: midnighters trilogy                   Similarity: -0.76\n",
      "Link: sword                                 Similarity: -0.76\n",
      "Link: category:novels by scott westerfeld   Similarity: -0.76\n",
      "Link: the uglies series                     Similarity: -0.79\n"
     ]
    }
   ],
   "source": [
    "find_similar('biography', link_weights, index_name = 'link', least = True, n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you not only know what books to look for, you know the books to avoid for a given category! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link: new york city                                 Similarity: 1.0\n",
      "Link: alfred a. knopf                               Similarity: 0.93\n",
      "Link: the new york times                            Similarity: 0.92\n",
      "Link: simon  &  schuster                            Similarity: 0.91\n",
      "Link: category:american novels adapted into films   Similarity: 0.9\n"
     ]
    }
   ],
   "source": [
    "find_similar('new york city', link_weights, index_name = 'link', n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model\n",
    "\n",
    "I was curious if training for the mean squared error as a regression problem was the ideal approach, so I also decided to experiment with a classifcation model. For this model, the negative examples receive a label of 0 and the loss function is binary cross entropy. The procedure for the neural network to learn the embeddings is exactly the same, only it will be optimizing for a slightly different measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_acc = book_embedding_model(50, classification = True)\n",
    "gen = generate_batch(pairs, n_positive, negative_ratio=2, classification = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous model might not have needed to train for so long. We can reduce the number of epochs and see if the result is affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model to learn embeddings\n",
    "h = model_acc.fit_generator(gen, epochs = 10, steps_per_epoch= len(pairs) // n_positive,\n",
    "                            verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll go through the same process, extracting the weights and finding similar books based on the embedding space representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('first_attempt_class.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_weights_acc = extract_weights('book_embedding', model_acc)\n",
    "book_weights_acc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('War and Peace', book_weights_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('The Fellowship of the Ring', book_weights_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_weights_acc = extract_weights('link_embedding', model_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('new york times', link_weights_acc, index_name = 'link', n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('category:1964 fantasy novels', link_weights_acc, index_name = 'link', n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations\n",
    "\n",
    "One of the coolest part about embeddings is that we can visualize them. First we have to take them from 50 dimensions down to either 3 or 2. We can do this using `pca`, `tsne`, or `umap`. We'll try both tsne and umap for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dim(weights, components = 3, method = 'tsne'):\n",
    "    if method == 'tsne':\n",
    "        return TSNE(components).fit_transform(weights)\n",
    "    elif method == 'umap':\n",
    "        return UMAP(n_components=components, metric = 'cosine', \n",
    "                    init = 'random', n_neighbors = 2).fit_transform(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_r = reduce_dim(book_weights_acc, components = 2, method = 'tsne')\n",
    "book_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.plot(book_r[:, 0], book_r[:, 1], 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_ru = reduce_dim(book_weights_acc, components = 2, method = 'umap')\n",
    "plt.plot(book_ru[:, 0], book_ru[:, 1], 'g.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Embeddings by Category\n",
    "\n",
    "This is a little hard to interpret. Let's take the 10 most popular categories, and color the image by the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [[link for link in book[2] if 'Category:' in link] for book in books]\n",
    "c_counts = count_items(list(chain(*categories)))\n",
    "list(c_counts.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll stick to the 10 most popular categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_to_include = list(c_counts.keys())[:10]\n",
    "c_to_include = [c.lower() for c in c_to_include]\n",
    "c_to_include[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of these categories have overlap and books can have multiple categories. Therefore we'll just take the first category for the book that is in the categories to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_include = []\n",
    "cs = []\n",
    "\n",
    "# Iterate through each book's categories\n",
    "for i, book_c in enumerate(categories):\n",
    "    # Iterate through books categories in the book\n",
    "    for c in book_c:\n",
    "        # If link is in c to include, record the index of book and category\n",
    "        if c.lower() in c_to_include:\n",
    "            idx_include.append(i)\n",
    "            cs.append(c.lower())\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to map the categories to integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints, cats = pd.factorize(cs)\n",
    "cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the embedding colored by the category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "# Plot embedding\n",
    "plt.scatter(book_ru[idx_include, 0], book_ru[idx_include, 1], \n",
    "            c = ints, cmap = plt.cm.tab10)\n",
    "\n",
    "# Add colorbar and appropriate labels\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_ticks(list(range(10)))\n",
    "cbar.set_ticklabels(cats)\n",
    "\n",
    "\n",
    "plt.xlabel('UMAP 1'); plt.ylabel('UMAP 2'); plt.title('UMAP Visualization of Book Embeddings');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't appear to be much separation between the categories. There are a lot of parameters to play around with in UMAP, and changing some of them might result in better clusters.\n",
    "\n",
    "As a last step, let's see the embedding labeled with the 10 books most often mentioned by other books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikilink_books = list(chain(*[[l for l in book[2] if l in book_index.keys()] for book in books]))\n",
    "wikilink_book_counts = count_items(wikilink_books)\n",
    "list(wikilink_book_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InteractiveShell.ast_node_interactivity = 'last'\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "# Plot embedding\n",
    "plt.scatter(book_ru[idx_include, 0], book_ru[idx_include, 1], \n",
    "            c = ints, cmap = plt.cm.tab10, alpha = 0.4)\n",
    "\n",
    "# Add colorbar and appropriate labels\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_ticks(list(range(10)))\n",
    "cbar.set_ticklabels(cats)\n",
    "\n",
    "\n",
    "plt.xlabel('UMAP 1'); plt.ylabel('UMAP 2'); plt.title('UMAP Visualization of Book Embeddings');\n",
    "\n",
    "for book in list(wikilink_book_counts.keys())[:10]:\n",
    "    \n",
    "    x, y = book_ru[book_index[book], 0], book_ru[book_index[book], 1];\n",
    "    _ = plt.scatter(x, y, s = 200, marker = '*', edgecolor = 'k')\n",
    "    _ = plt.text(x, y, book, fontsize = 12);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Book Nearest Neighbors\n",
    "\n",
    "To get a better sense of which books are located where, we can plot a book along with its nearest neighbors. These will be the nearest neighbors in the original embedding space, so they are not necessarily the closest in the reduced dimension representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist, closest = find_similar('War and Peace', book_weights_acc, 'book', n = 10, return_dist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_closest(item, weights, index_name, n, plot_data):\n",
    "    \"\"\"Plot n most closest items to item\"\"\"\n",
    "    \n",
    "    # Find the closest items\n",
    "    dist, closest = find_similar(item, weights, index_name, n, return_dist=True)\n",
    "    \n",
    "    # Choose mapping for look up\n",
    "    if index_name == 'book':\n",
    "        index = book_index\n",
    "        rindex = index_book\n",
    "    elif index_name == 'link':\n",
    "        index = link_index\n",
    "        rindex = index_link \n",
    "    \n",
    "    plt.figure(figsize = (10, 8))\n",
    "    plt.rcParams['font.size'] = 14\n",
    "    \n",
    "    # Limit distances\n",
    "    dist = dist[closest]\n",
    "    \n",
    "    # Plot all of the data\n",
    "    plt.scatter(plot_data[:, 0], plot_data[:, 1], alpha = 0.1, color = 'goldenrod')\n",
    "    \n",
    "    # Plot the item\n",
    "    plt.scatter(plot_data[closest[-1], 0], plot_data[closest[-1], 1], s = 600, edgecolor = 'k', color = 'b')\n",
    "    \n",
    "    # Plot the closest items\n",
    "    plt.scatter(plot_data[closest, 0], plot_data[closest, 1], \n",
    "                c = dist, cmap = plt.cm.copper, s = 200, alpha = 1, marker = '*')\n",
    "    \n",
    "    # Colorbar management\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label(f'{index_name.capitalize()} and Cosine Distance')\n",
    "    cbar.set_ticks(np.linspace(dist.min(), dist.max(), 10))\n",
    "    cbar.set_ticklabels([f'{rindex[idx]}: {distance:.2}' for idx, distance in zip(closest, dist)])\n",
    "    \n",
    "    # Labeling\n",
    "    plt.xlabel('UMAP 1'); plt.ylabel('UMAP 2'); plt.title(f'Most Similar {index_name.capitalize()}s to {item}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_closest('War and Peace', book_weights_acc, 'book', 10, book_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_closest('A Brief History of Time', book_weights_acc, 'book', 10, book_ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Links\n",
    "\n",
    "Let's look at reducing the dimension of the embedding for the links. We'll start with only the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_include = [idx for link, idx in link_index.items() if 'category:' in link]\n",
    "link_ru = reduce_dim(link_weights_acc[idx_to_include], components = 2, method = 'umap')\n",
    "link_ru.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll get the 10 most popular categories so we can plot them highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = []\n",
    "for link in cats:\n",
    "    idx.append(int(np.where(np.array(idx_to_include) == link_index[link.lower()])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 12))\n",
    "plt.scatter(link_ru[:, 0], link_ru[:, 1], alpha = 0.6)\n",
    "\n",
    "for i in idx:\n",
    "    x, y = link_ru[i, 0], link_ru[i, 1]\n",
    "    s = index_link[idx_to_include[i]].split(':')[-1]\n",
    "    _ = plt.text(x, y, s, fontsize = 18);\n",
    "    \n",
    "plt.xlabel('UMAP 1'); plt.ylabel('UMAP 2'); plt.title('UMAP Visualization of Wikilinks Embedding');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_closest('category:american science fiction novels', link_weights_acc, 'link', 10, link_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_closest('category:novels set in new york city', link_weights_acc, 'link', 10, link_ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go ahead and reduce the dimension of all the links. Then we can do the same as before, plotting the closest links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_ru = reduce_dim(link_weights_acc, components = 2, method = 'umap')\n",
    "link_ru.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_closest('new york times', link_weights_acc, 'link', 10, link_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_closest('james joyce', link_weights_acc, 'link', 10, link_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_closest('margaret atwood', link_weights_acc, 'link', 10, link_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_closest('george eliot', link_weights_acc, 'link', 10, link_ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Embeddings for Visualization\n",
    "\n",
    "We can save these embeddings for visualization in [projector.tensorflow.org](https://projector.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "with open('link_names.tsv', 'w' , encoding = 'utf-8') as fout:\n",
    "    for l in link_index.keys():\n",
    "        fout.write(str(l))\n",
    "        fout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "with open('book_names.tsv', 'w' , encoding = 'utf-8') as fout:\n",
    "    for l in book_index.keys():\n",
    "        fout.write(str(l))\n",
    "        fout.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The book embeddings and link embeddings can be saved as tab separated files (the format specified by the online application). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('book_embedding.tsv', book_weights_acc, delimiter='\\t')\n",
    "np.savetxt('link_embedding.tsv', link_weights_acc, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_embedding = np.loadtxt('book_embedding.tsv', delimiter = '\\t')\n",
    "book_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, we built an effective book recommendation system using the outgoing Wikipedia links from all books articles on Wikipedia. We did this by creating embeddings of the books and the links with the assumption that books linking to similar pages are similar to one another. We saw how to inspect the embeddings in order to find the closest books or links and also how to visualize the embeddings which sometimes can show us interestings clusterings. The next step is to visualize these embeddings in an interactive tool, or use additional information to refine the book recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
