{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Downloading and Parsing Wikipedia Articles\n",
    "\n",
    "In this notebook, we will download all of the latest wikipedia articles. After getting the data, we'll work on making sense of it using data science!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for Wikipedia Dump\n",
    "\n",
    "To start, we make a request to the [Wikimedia dump](https://dumps.wikimedia.org/) of Wikipedia. We'll search through `enwiki` which has the [English language dumps](https://dumps.wikimedia.org/enwiki/) of wikipedia. This first request finds the available recent dumps and lists them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://dumps.wikimedia.org/enwiki/'\n",
    "index = requests.get(base_url).text\n",
    "soup_index = BeautifulSoup(index, 'html.parser')\n",
    "\n",
    "# Find the links that are dates of dumps\n",
    "dumps = [a['href'] for a in soup_index.find_all('a') if \n",
    "         a.has_attr('href')]\n",
    "dumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line of code finds the html of the dump for the first of September. If there is a more recent version available, feel free to use that instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_url = base_url + '20180901/'\n",
    "\n",
    "# Retrieve the html\n",
    "dump_html = requests.get(dump_url).text\n",
    "dump_html[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can parse through the html text to find the available files for download. We will focus only on the most recent revision of the articles themselves. It is possible to get the past history of articles, the edits, the discussion, and metadata, but the articles themselves provide us with more than enough data! \n",
    "\n",
    "For more information on the available downloads, take a look at the [Wikimedia dump](https://dumps.wikimedia.org/) or on [Wikipedia itself](https://en.wikipedia.org/wiki/Wikipedia:Database_download#English-language_Wikipedia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding What to Download\n",
    "\n",
    "The most recent revision of every single article (what we are after) is available in a single compressed file as `pages-articles.xml.bz2`. However, we'll download the articles in smaller chunks so that we can then process it in parallel (rather than all sequentially). The single file - which is compressed XML (using bz2) - is over 15 GB while the smaller chunks - of which there are more than 50 - are several hundred megabytes each. Since we want to find every single book on Wikipedia, we'll go with this particular dump. \n",
    "\n",
    "If you only want to find a specific article, then the `pages-article-multistream.xml.bz2` may be a better choice. There is an `index` for the multistream version that means you can locate a specific article within the compressed file without having to search through the entire file. For more information refer to the \"Should I get Multistream?\" section on [this Wikipedia page](https://en.wikipedia.org/wiki/Wikipedia:Database_download#).\n",
    "\n",
    "To find all the individual files, we'll search through the html from the dump identified earlier. We'll look for any files that have `pages_articles` in the text. To parse through html (or other markup languages), we can use Beautiful Soup. This makes searching for particular tags or classes (such as `file` below) very efficient. If you are doing any web scraping, this will be a very useful library to learn! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a soup\n",
    "soup_dump = BeautifulSoup(dump_html, 'html.parser')\n",
    "\n",
    "# Find li elements with the class file\n",
    "soup_dump.find_all('li', {'class': 'file'}, limit = 10)[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to search for only the files containing `pages-articles` to get only the recent versions of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "\n",
    "# Search through all files\n",
    "for file in soup_dump.find_all('li', {'class': 'file'}):\n",
    "    text = file.text\n",
    "    # Select the relevant files\n",
    "    if 'pages-articles' in text:\n",
    "        files.append((text.split()[0], text.split()[1:]))\n",
    "        \n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want the partitioned files so we can further refine our selection to those with `xml-p`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_download = [file[0] for file in files if '.xml-p' in file[0]]\n",
    "files_to_download[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Wikipedia Data\n",
    "\n",
    "Now we need to actually download the data. This can be done using the keras `get_file` utility which downloads the specified file at the specified url. If we already have the entire dataset downloaded, then we don't want to download it again! For that reason we first use a check to see if the data exists.\n",
    "\n",
    "The default download directory for keras is `~/.keras/datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from keras.utils import get_file\n",
    "\n",
    "keras_home = '/home/ubuntu/.keras/datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = []\n",
    "file_info = []\n",
    "\n",
    "# Iterate through each file\n",
    "for file in files_to_download:\n",
    "    path = keras_home + file\n",
    "    \n",
    "    # Check to see if the path exists\n",
    "    if not os.path.exists(keras_home + file):\n",
    "        print('Downloading')\n",
    "        # If not, download the file\n",
    "        data_paths.append(get_file(file, dump_url))\n",
    "        # Find the file size in MB\n",
    "        file_size = os.stat(path).st_size / 1e6\n",
    "        \n",
    "        # Find the number of articles\n",
    "        file_articles = int(file.split('p')[-1].split('.')[-2]) - int(file.split('p')[-2])\n",
    "        file_info.append((file, file_size, file_articles))\n",
    "        \n",
    "    # Otherwise extract information\n",
    "    else:\n",
    "        data_paths.append(path)\n",
    "        # Find the file size in MB\n",
    "        file_size = os.stat(path).st_size / 1e6\n",
    "        \n",
    "        # Find the number of articles\n",
    "        file_number = int(file.split('p')[-1].split('.')[-2]) - int(file.split('p')[-2])\n",
    "        file_info.append((file.split('-')[-1], file_size, file_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the largest file? We can use `sorted` to sort by the file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(file_info, key = lambda x: x[1], reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure what the `p` numbers mean at the end of the file name. It must have something to do with the partitioning of the data. At first I thought it was the number of articles in the partition, but the English language Wikipedia only has 5.8 million article and these numbers go up to 58 million. If we subtract the second number from the first, the most common number is 1.5 million. Maybe this has something to do with the number of bytes in the file? I'm not sure, but I would appreciate if anyone knows! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(file_info, key = lambda x: x[2], reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the largest files are about 400 MB compressed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(file_info)} partitions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about putting this info into a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "file_df = pd.DataFrame(file_info, columns = ['file', 'size (MB)', 'articles']).set_index('file')\n",
    "file_df['size (MB)'].plot.bar(color = 'red', figsize = (12, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total there are 5.7 million articles contained in these files. In a way, it's surprising that the extent of all human knowledge (okay I'm exaggerating somewhat) is now contained on our computer! \n",
    "\n",
    "#### Size of Wikipedia \n",
    "\n",
    "For some interesting reading, check out the following Wikipedia Articles:\n",
    "\n",
    "* [Size of Wikipedia](https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia)\n",
    "* [Modelling Growth of Wikipedia](https://en.wikipedia.org/wiki/Wikipedia:Modelling_Wikipedia%27s_growth#Data_set_for_number_of_articles)\n",
    "* [Size Comparisons of Wikipedia to other Encyclopedias](https://en.wikipedia.org/wiki/Wikipedia:Size_comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The total size of files on disk is {file_df['size (MB)'].sum() / 1e3} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we could download this all at once with the `pages-articles.xml.bz2` file, but then we would have to parse it sequentially using only one process. Instead, because we have partitioned data, we can iterate over the files in parallel (as we'll see later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Through the Data\n",
    "\n",
    "Together, the files take up 15.4 GB. Decompressed as xml, it's close to 50 GB. We could decompress each file into XML and then parse through it, but we can also parse through a decompressed file one line at a time. If we are concered about disk space, this is a better option. Working through the compressed file one line at a time also might be the only option in cases where the individual files are too large to fit in memory. \n",
    "\n",
    "We'll start by working through one of the files and then develop functions that we can run on all of the files. Because we downloaded the data in chunks, we'll be able to parallelize the parsing operations.\n",
    "\n",
    "To get started, make sure you have `bzcat` installed on your system. The [`bzcat` utility] (http://www.qnx.com/developers/docs/6.5.0SP1.update/com.qnx.doc.neutrino_utilities/b/bzcat.html) is a command line program that decompresses a bz2 compressed file and sends the contents to standard out. To go through the files one line at a time, we simply iterate over the command to decompress the file. We call the `bzcat` command using `subprocess` which is often used to execute system commands in Python.\n",
    "\n",
    "Another option for decompressing the file is `bz2`. However, in tests (see below), I found this to be much slower than `bzcat`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import subprocess\n",
    "\n",
    "data_path = data_paths[15]\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "\n",
    "lines = []\n",
    "for i, line in enumerate(bz2.BZ2File(data_path, 'r')):\n",
    "    lines.append(line)\n",
    "    if i > 1e6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "\n",
    "lines = []\n",
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                         stdin = open(data_path), \n",
    "                         stdout = subprocess.PIPE).stdout):\n",
    "    lines.append(line)\n",
    "    if i > 1e6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `subprocess` + `bzcat` approach is nearly twice as fast. Let's run this again and see what kind of data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                         stdin = open(data_path), \n",
    "                         stdout = subprocess.PIPE).stdout):\n",
    "    lines.append(line)\n",
    "    if i > 5e5:\n",
    "        break\n",
    "        \n",
    "lines[-165:-109]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a complete article. We could go through each article and extract out the information using regular expression, but that would be extremely inefficient. Instead, we can use an xml parser to extract precisely the information we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing XML\n",
    "\n",
    "In order to get useful information from this data, we have to parse it on two levels.\n",
    "\n",
    "1. Extract the titles and article text from the XML\n",
    "2. Extract relevant information from the article text\n",
    "\n",
    "To solve the first problem, we'll use a tool purpose built for the task of parsing XML, SAX: The Simple API for XML. [The documentation](http://pyxml.sourceforge.net/topics/howto/section-SAX.html) is a little difficult to follow, but the basic idea is that we can use SAX to search through the XML and select elements based on the tag. (If you need an introduction to XML, I'd highly recommend starting [here](https://www.w3schools.com/xml/default.asp)).\n",
    "\n",
    "For example, if we have the follow XML element, we want to extract the text that occurs between the `<title>` tags:\n",
    "\n",
    "`<title>Carroll Knicely</title>`\n",
    "\n",
    "Likewise, if we have the content of an article like below, we want to extract the text that occurs between the `<text>` tags. \n",
    "\n",
    "```XML\n",
    "<text xml:space=\"preserve\">\\'\\'\\'Carroll F. Knicely\\'\\'\\' (born c. 1929 in [[Staunton, Virginia]] - died November 2, 2006 in [[Glasgow, Kentucky]]) was [[Editing|editor]] and [[Publishing|publisher]] of the \\'\\'[[Glasgow Daily Times]]\\'\\' for nearly 20 years (and later, its owner) and served under three [[Governor of Kentucky|Kentucky Governors]] as commissioner and later Commerce Secretary.\\n'\n",
    "</text>\n",
    "```\n",
    "\n",
    "We'll use the SAX parser to do exactly that: find the titles and text content of the articles. Then, we can pass the text to another parser to extract information from the article. \n",
    "\n",
    "Explaining how SAX works is a little more difficult than just showing, so I'll present the code and show some examples. A SAX parser requires a content handler, which is the only code we need to write. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.sax\n",
    "\n",
    "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
    "    \"\"\"Parse through XML data using SAX\"\"\"\n",
    "    def __init__(self):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._current_tag = None\n",
    "        self._pages = []\n",
    "\n",
    "    def characters(self, content):\n",
    "        \"\"\"Characters between opening and closing tags\"\"\"\n",
    "        if self._current_tag:\n",
    "            self._buffer.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        \"\"\"Opening tag of element\"\"\"\n",
    "        if name in ('title', 'text', 'timestamp'):\n",
    "            self._current_tag = name\n",
    "            self._buffer = []\n",
    "\n",
    "    def endElement(self, name):\n",
    "        \"\"\"Closing tag of element\"\"\"\n",
    "        if name == self._current_tag:\n",
    "            self._values[name] = ' '.join(self._buffer)\n",
    "\n",
    "        if name == 'page':\n",
    "            self._pages.append((self._values['title'], self._values['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a `handler` object of the `WikiXMLHandler` class. Then we pass the handler in as the content handler to a SAX `parser`. Basically, we are overriding a few of the default SAX `ContentHandler` methods in order to do what we want: find the titles and texts in the XML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object for handling xml\n",
    "handler = WikiXmlHandler()\n",
    "\n",
    "# Parsing object\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "handler._pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work through a single article. We `feed` in one line of XML at a time to the `parser`. It searches the XML for the tags using the methods in the handler. The correct data is then stored in the handler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in lines[-165:-109]:\n",
    "    parser.feed(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler._pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully extracted one page! Once we have the page text, we need to process it as well to find the information we want. We'll write that function next. First, let's see this process again, this time finding three different articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object for handling xml\n",
    "handler = WikiXmlHandler()\n",
    "\n",
    "# Parsing object\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                         stdin = open(data_path), \n",
    "                         stdout = subprocess.PIPE).stdout):\n",
    "    parser.feed(line)\n",
    "    \n",
    "    # Stop when 3 articles have been found\n",
    "    if len(handler._pages) > 2:\n",
    "        break\n",
    "        \n",
    "print([x[0] for x in handler._pages])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to process the actual text of the article. For this, we will turn to the `mwparserfromhell` library. [This library](https://github.com/earwig/mwparserfromhell) is custom made for parsing `MediaWiki` wikicode which is a standard that includes Wikipedia articles. [MediaWiki](https://www.mediawiki.org/wiki/MediaWiki) is used by Wikipedia and numerous other projects and provides a relatively standardized template for creating wiki pages. Thanks to this standardization, we can using a custom built parser to go through the articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Articles\n",
    "\n",
    "The best way to figure out how to parse an article is simply to do it! Let's work through one article.\n",
    "\n",
    "First we'll find a number of articles using the `WikiXmlHandler` and a SAX parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object for handling xml\n",
    "handler = WikiXmlHandler()\n",
    "\n",
    "# Parsing object\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                         stdin = open(data_path), \n",
    "                         stdout = subprocess.PIPE).stdout):\n",
    "    parser.feed(line)\n",
    "    \n",
    "    # Stop when 50 articles have been found\n",
    "    if len(handler._pages) > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to do is pass the Wikipedia article text to the `mwparserfromhell`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwparserfromhell \n",
    "\n",
    "print(handler._pages[6][0])\n",
    "\n",
    "# Create the wiki article\n",
    "wiki = mwparserfromhell.parse(handler._pages[6][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work through the article for KENZ (FM). It might be helpful to pull up the [Wikipedia page alongside](https://en.wikipedia.org/wiki/KENZ_(FM)) so you can see what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(wiki))\n",
    "wiki[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a normal string, but in fact, it's a `mwparserfromhell.wikicode.Wikicode` object with many different methods for sorting through the content. For example, we can find all the internal links (those that go to other wikipedia pages) using `wiki.filter_wikilinks()`. This will give us the `title` of the Wikipedia article linked to as well as the `text` of the link. We'll extract just the titles of the articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikilinks = [x.title for x in wiki.filter_wikilinks()]\n",
    "print(f'There are {len(wikilinks)} wikilinks.')\n",
    "wikilinks[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't get the discussion or the edits around the articles, so we won't be able to find this information. However, if you do decide to grab comments, edits, revisions, etc., you can use `mwparserfromhell` to extract all of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.filter_arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.filter_comments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out everything you can do with `mwparserfromhell`, [read the docs](https://mwparserfromhell.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the external links (those that go outside of Wikipedia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_links = [(x.title, x.url) for x in wiki.filter_external_links()]\n",
    "print(f'There are {len(external_links)} external links.')\n",
    "external_links[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search through the text for specific words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contemporary = wiki.filter(matches = 'contemporary')\n",
    "contemporary[1], type(contemporary[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get a cleaner version of just the text, you can call `wiki.strip_code()` and then add another `strip`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.strip_code().strip()[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's quite a bit more you can do with this class, so explore the options if you have a project in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Templates\n",
    "\n",
    "The easiest way to filter articles to a category (at least that I've found) is through the use of templates. These are standardized formats for information. For example, one template on the `KENZ (FM)` radio station page looks like this:\n",
    "\n",
    "![Radio Station Infobox](https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/images/radio_template.PNG?raw=true)\n",
    "\n",
    "### Infobox Templates\n",
    "\n",
    "These particular template is called an `Infobox`. There are many different infoboxes, each one for a different category such as films or books. __The easiest way to filter articles to one category is by using the `Infobox` template for that category__.\n",
    "You can read about infoboxes [here](https://en.wikipedia.org/wiki/Help:Infobox) or look at the list [here](https://en.wikipedia.org/wiki/Wikipedia:List_of_infoboxes)\n",
    "\n",
    "Let's take a look at the templates for this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = wiki.filter_templates()\n",
    "print(f'There are {len(templates)} templates.')\n",
    "for template in templates:\n",
    "    print(template.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of different templates. In this case, the Infobox is name `Infobox radio station`. If we wanted to find all of the radio station articles, then the easiest way would be to search every article for this template. We can search for a specific template within an article as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox = wiki.filter_templates(matches = 'Infobox radio station')[0]\n",
    "infobox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attributes of the infobox can be accessed and put into a dictionary using the `name` and the `value`. To clean things up, we first strip the code and then strip whitespace and escape characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information = {param.name.strip_code().strip(): param.value.strip_code().strip() for param in infobox.params}\n",
    "information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Infobox` templates provide us with a consistent method for finding a category of article. We simply need to search the article text for the `Infobox` of the category of article we want. \n",
    "\n",
    "Moreover, the information in the template could be useful for say building a set of features in a predictive model. In this project, we aren't going to be working with the text of the articles - that's an entirely separate undertaking - but we can use information such as that found in the `Infobox` or the `wikilinks` to build a recommendation engine. \n",
    "\n",
    "## Searching for Books\n",
    "\n",
    "Now that we have an understanding of how to parse an article and find articles belonging to a certain category, we can start searching for what we want: all the books on Wikipedia! The books can be identified because they use an Infobox book template.\n",
    "\n",
    "![Infobox book](https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/images/book_template.PNG?raw=true)\n",
    "\n",
    "We simply have to filter the article text for the `Infobox book` template, and if it's present, store the information. If not, then we move to the next article. The function below is designed to find and return book articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_article(title, text, timestamp, template = 'Infobox book | infobox book'):\n",
    "    \"\"\"Process a wikipedia article looking for template\"\"\"\n",
    "    \n",
    "    # Create a parsing object\n",
    "    wikicode = mwparserfromhell.parse(text)\n",
    "    \n",
    "    # Search through templates for the book template\n",
    "    matches = wikicode.filter_templates(matches = template)\n",
    "    \n",
    "    if matches:\n",
    "        # Extract information from infobox\n",
    "        properties = {param.name.strip_code().strip(): param.value.strip_code().strip() \n",
    "                      for param in matches[0].params\n",
    "                      if param.value.strip_code().strip()}\n",
    "        \n",
    "        # Extract internal wikilinks\n",
    "        wikilinks = [x.title.strip_code().strip() for x in wikicode.filter_wikilinks()]\n",
    "        # Extract external links\n",
    "        exlinks = [x.url.strip_code().strip() for x in wikicode.filter_external_links()]\n",
    "        return (title, properties, wikilinks, exlinks, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = process_article('KENZ (FM)', wiki, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No such luck with this article searching for books. We can modify the search to fit our template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = process_article('KENZ (FM)', wiki, None, template = 'Infobox radio station | infobox radio station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[0], r[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse the text from each article, we slightly modify the `Handler` class. This time, in the `endElement` function, if the article ends (the tag is `page`) then we send the title and the contents (`text`) to the `process_article` function. This function will return either nothing if it doesn't find a book, or the book properties, Wikilinks, and external links if it does find a book. These will be added as a list to the `handler`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
    "    \"\"\"Parse through XML data using SAX\"\"\"\n",
    "    def __init__(self):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._current_tag = None\n",
    "        self._books = []\n",
    "        self._article_count = 0\n",
    "\n",
    "    def characters(self, content):\n",
    "        \"\"\"Characters between opening and closing tags\"\"\"\n",
    "        if self._current_tag:\n",
    "            self._buffer.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        \"\"\"Opening tag of element\"\"\"\n",
    "        if name in ('title', 'text', 'timestamp'):\n",
    "            self._current_tag = name\n",
    "            self._buffer = []\n",
    "\n",
    "    def endElement(self, name):\n",
    "        \"\"\"Closing tag of element\"\"\"\n",
    "        if name == self._current_tag:\n",
    "            self._values[name] = ' '.join(self._buffer)\n",
    "\n",
    "        if name == 'page':\n",
    "            self._article_count += 1\n",
    "            # Search through the page to see if the page is a book\n",
    "            book = process_article(**self._values)\n",
    "            # Append to the list of books\n",
    "            if book:\n",
    "                self._books.append(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below stops when we've found 3 books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object for handling xml\n",
    "handler = WikiXmlHandler()\n",
    "\n",
    "# Parsing object\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                         stdin = open(data_path), \n",
    "                         stdout = subprocess.PIPE).stdout):\n",
    "    parser.feed(line)\n",
    "    \n",
    "    # Stop when 3 articles have been found\n",
    "    if len(handler._books) > 2:\n",
    "        break\n",
    "        \n",
    "print(f'Searched through {handler._article_count} articles to find 3 books.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we've got! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler._books[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each book, we have the title, the information contained in the `Infobox`, the internal Wikilinks, the external links, and the timestamp of the last edit. Using just this information, we'll be able to build a fairly robust book recommendation system! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how long it would take to search through just one partition. Uncompressed, the size of this partition is 1.5 GB with over 24 million lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncompress the file if not already uncompressed\n",
    "if not os.path.exists('/home/ubuntu/.keras/datasets/p15.xml'):\n",
    "    subprocess.call(['bzcat /home/ubuntu/.keras/datasets/enwiki-20180901-pages-articles15.xml-p7744803p9244803.bz2 >> p15.xml'],\n",
    "                    shell = True)\n",
    "else:\n",
    "    print('Already uncompressed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc /home/ubuntu/.keras/datasets/p15.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "# Object for handling xml\n",
    "handler = WikiXmlHandler()\n",
    "\n",
    "# Parsing object\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "# Parse the entire file\n",
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                         stdin = open(data_path), \n",
    "                         stdout = subprocess.PIPE).stdout):\n",
    "    if (i + 1) % 10000 == 0:\n",
    "        print(f'Processed {i + 1} lines so far.', end = '\\r')\n",
    "    try:\n",
    "        parser.feed(line)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    \n",
    "end = timer()\n",
    "books = handler._books\n",
    "\n",
    "print(f'\\nSearched through {handler._article_count} articles.')\n",
    "print(f'\\nFound {len(books)} books in {round(end - start)} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we multiply the time to process one partition, about 1000 seconds, by the number of partitions, 55, we get 55,000 seconds to process all the files one at a time. This amounts to 15 hours! We'll see if we can do better using multiprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to File\n",
    "\n",
    "The best way to save the information is as `ndjson`. We can save and then load back in the books using `json`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save list of books\n",
    "with open('p15_books.ndjson', 'wt') as fout:\n",
    "    for l in books:\n",
    "        fout.write(json.dumps(l) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_in = []\n",
    "\n",
    "# Read in list of books\n",
    "with open('p15_books.ndjson', 'rt') as fin:\n",
    "    for l in fin.readlines():\n",
    "        books_in.append(json.loads(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_in[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Operations in Parallel\n",
    "\n",
    "Instead of parsing through the files one at a time, we want to process several of them at once. We can do this using parallelization, either through multithreading or multiprocessing.\n",
    "\n",
    "\n",
    "## Multithreading and Multiprocessing\n",
    "\n",
    "Multithreading and multiprocessing are ways to carry out many tasks simulataneously. We have a number of files on disk, each of which needs to be parsed in the same way. A naive approach would be to parse one file at a time, but that is not taking full advantage of our resources. Instead, we can use either multithreading or multiprocessing to parse many files at the same time, speeding up the overall process. \n",
    "\n",
    "Learning multithreading / multiprocessing is essential for making your data science workflows more efficient. I'd recommend [this article](https://medium.com/@bfortuner/python-multithreading-vs-multiprocessing-73072ce5600b) to get started with the concepts.\n",
    "\n",
    "Generally multithreading works better (is faster) for input / output bound tasks, such as reading in files or making requests. Multiprocessing works better (is faster) for cpu bound tasks. Due to the [global interpreter lock](https://realpython.com/python-gil/), only one thread can run at a time in Python for cpu intensive tasks (such as doing mathematical operations or machine learning) even on a multithreaded / multicore system. Running multiple processes does not have this issue because processes do not share memory. In some cases, you cannot use multiple processes because the tasks need to share memory. However, in our case, each file can be processed independently of the others. For that reason, we can use either multithreading or mulitprocessing.\n",
    "\n",
    "I've found the best method for determining the optimal method and parameters is to try out a number of different options. The code for testing multithreding and mutltiprocessing appears after this next section. When I ran the testing code first and then tried to run the actual code, I ran into out of memory errors so I put it at the end.\n",
    "\n",
    "__The final process of searching for all books on Wikipedia is run in parallel using 16 processes__. This was the fastest method in my benchmarking. Each process is run on a different core because the machine I used had 16 cores. The best options will depend on your set-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Parallel Code\n",
    "\n",
    "To run an operation in parallel, we need a `service` and `tasks`. A `service` can just be a function that we need to run many times and the `tasks` are the different arguments to that function. For our purposes, the service is the `find_books` function that parses a complete compressed xml file for the books and the tasks are all of the compressed xml files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to write a function that takes in a file and returns a list of the book articles. We already have all the parts defined so we can just put them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "\n",
    "def find_books(data_path, limit = None, save = True):\n",
    "    \"\"\"Find all the book articles from a compressed wikipedia XML dump.\n",
    "       `limit` is an optional argument to only return a set number of books.\n",
    "        If save, books are saved to partition directory based on file name\"\"\"\n",
    "\n",
    "    # Object for handling xml\n",
    "    handler = WikiXmlHandler()\n",
    "\n",
    "    # Parsing object\n",
    "    parser = xml.sax.make_parser()\n",
    "    parser.setContentHandler(handler)\n",
    "\n",
    "    # Iterate through compressed file\n",
    "    for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                             stdin = open(data_path), \n",
    "                             stdout = subprocess.PIPE).stdout):\n",
    "        try:\n",
    "            parser.feed(line)\n",
    "        except StopIteration:\n",
    "            break\n",
    "            \n",
    "        # Optional limit\n",
    "        if limit is not None and len(handler._books) >= limit:\n",
    "            return handler._books\n",
    "    \n",
    "    if save:\n",
    "        partition_dir = '/data/wiki/partitions/'\n",
    "        # Create file name based on partition name\n",
    "        p_str = data_path.split('-')[-1].split('.')[-2]\n",
    "        out_dir = partition_dir + f'{p_str}.ndjson'\n",
    "\n",
    "        # Open the file\n",
    "        with open(out_dir, 'w') as fout:\n",
    "            # Write as json\n",
    "            for book in handler._books:\n",
    "                fout.write(json.dumps(book) + '\\n')\n",
    "        \n",
    "        print(f'{len(os.listdir(partition_dir))} files processed.', end = '\\r')\n",
    "\n",
    "    # Memory management\n",
    "    del handler\n",
    "    del parser\n",
    "    gc.collect()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need a list of services. This is simply all the partitioned data files we want to send to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = [keras_home + file for file in os.listdir(keras_home) if 'xml-p' in file]\n",
    "len(partitions), partitions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool \n",
    "import tqdm \n",
    "\n",
    "# List of lists to single list\n",
    "from itertools import chain\n",
    "\n",
    "# Sending keyword arguments in map\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for Every Book on Wikipedia\n",
    "\n",
    "The code below simply searches all of Wikipedia for every book article! The `tqdm` library is used to track progress (the resulting widgets may not show up correclty in a static Jupyter Notebook).\n",
    "\n",
    "We create a `Pool` with the number of processes and then call `map` with the `(service, tasks)`. The books for each partition are saved to a json file on disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pool of workers to execute processes\n",
    "pool = Pool(processes = 16)\n",
    "\n",
    "start = timer()\n",
    "\n",
    "# Map (service, tasks), applies function to each partition\n",
    "results = pool.map(find_books, partitions)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "end = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Code with Progress Bar\n",
    "\n",
    "If you would like a progress bar to appear in your Jupyter Notebook as you run the operation, you can use the code below. This wraps `tqdm` around a call to `imap_unordered`. This does the same basic job as `map` except the results are returned as they finished instead of all at once. The end result will still be the exact same, you just get a progress bar as the computation runs. There are some situations where you'd want to use `imap` instead of `map`, and [this Stack Overflow answer](https://stackoverflow.com/a/26521507/5755357) does a great job of explaining the differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = timer()\n",
    "# pool = Pool(processes = 8)\n",
    "# results = []\n",
    "\n",
    "# # Run partitions in parallel\n",
    "# for x in tqdm.tqdm_notebook(pool.imap_unordered(find_books, partitions), total = len(partitions)):\n",
    "#     results.append(x)\n",
    "    \n",
    "# pool.close()\n",
    "# pool.join()\n",
    "\n",
    "# end = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Data Together\n",
    "\n",
    "After creating the separate files, each one containing the books from one partition, we can join them together into a single list, this time using `multithreading`. Again, we'll need a service, this time the function `read_data`, and tasks, all the `saved_files`, each of which contains the books for one partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    \"\"\"Read in json data from `file_path`\"\"\"\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    # Open the file and load in json\n",
    "    with open(file_path, 'r') as fin:\n",
    "        for l in fin.readlines():\n",
    "            data.append(json.loads(l))\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `multithreading` instead of `multiprocessing`, we import `Pool` from the [`multiprocessing.dummy` module](https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing.dummy). Then we create a `Threadpool` of workers and `map` the `tasks` to the `service`. This will return a lists of lists that we can flatten to a single list using `chain` from itertools.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import Pool as Threadpool\n",
    "import itertools\n",
    "\n",
    "start = timer()\n",
    "\n",
    "# List of files to read in\n",
    "saved_files = ['/data/wiki/partitions/' + x for x in os.listdir('/data/wiki/partitions/')]\n",
    "\n",
    "# Create a threadpool for reading in files\n",
    "threadpool = Threadpool(processes = 10)\n",
    "\n",
    "# Read in the files as a list of lists\n",
    "results = threadpool.map(read_data, saved_files)\n",
    "\n",
    "# Flatten the list of lists to a single list\n",
    "book_list = list(chain(*results))\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print(f'Found {len(book_list)} books in {round(end - start)} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what kind of speed ups this offered us, we can compare the results to reading in the files sequentially. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "start = timer()\n",
    "\n",
    "for file in saved_files:\n",
    "    with open(f'/data/wiki/partitions/{file}', 'r') as fin:\n",
    "        for l in fin.readlines():\n",
    "            results.append(l)\n",
    "\n",
    "end = timer()\n",
    "book_list = list(chain(*results))\n",
    "print(f'Found {len(book_list)} books in {round(end - start)} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists()\n",
    "\n",
    "with open('found_books.ndjson', 'wt') as fout:\n",
    "    for book in book_list:\n",
    "         fout.write(json.dumps(book) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Multithreading vs Multiprocessing\n",
    "\n",
    "I've found the only way to know which to use is to do a test of both and see which is faster. Likewise, when it comes to choosing the number of threads, doing an empirical test seems to be the most effective method for maximizing efficiency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Multiprocess\n",
    "\n",
    "First we'll test multiprocessing. To make the comparisons feasible, we'll limit the search to finding 10 books in each file. We'll compare multiprocessing with 8 cores to 16 cores to see if we can achieve a double speed up!\n",
    "\n",
    "The code is wrapped in a `tqdm` call which just displays a progress bar. This might not render correctly in the static version of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_speed(pool):\n",
    "    \"\"\"Time how long \"\"\"\n",
    "    start = timer()\n",
    "        \n",
    "    # Need to pass in keyword limit argument\n",
    "    map_find_books = partial(find_books, limit = 10, save = False)\n",
    "    \n",
    "    results = []\n",
    "    for x in tqdm.tqdm_notebook(pool.imap_unordered(map_find_books, partitions), total = len(partitions)):\n",
    "        results.append(x)\n",
    "        \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    end = timer()\n",
    "\n",
    "    book_list = list(chain(*results))\n",
    "    print(f'Found {len(book_list)} books in {round(end - start)} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(processes = 8)\n",
    "test_speed(pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try again with 16 processes. Theoretically this should halve the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(processes = 16)\n",
    "test_speed(pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using double the number of processors is faster, although not quite twice as fast. If this is the only task you are going to be running, it makes sense to use all of your cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Multithreading\n",
    "\n",
    "The syntax for using multithreading is exactly the same. I have no idea how many threads to use so I just tried 2 arbitrary numbers! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "thread_pool = ThreadPool(processes = 10)\n",
    "test_speed(thread_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_pool = ThreadPool(processes = 20)\n",
    "test_speed(thread_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threading appears to be much slower than multiprocessing. For a real run on all the data, we'll should use all available cores on our machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, we saw how to download the most recent version of every article on English language Wikipedia. Not only are we able to download all of the information, but we also saw some tools for processing this information to extract the data that we need. Wikipedia is an incredible resource, not only for doing your school projects, but also for exploring techniques in data science. In future notebooks, we'll look at how to build a book recommendation engine based on the data we've collected here. Gathering the data is one thing, but eventually, we want to be able to accomplish useful tasks with this data! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
